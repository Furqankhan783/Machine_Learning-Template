{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442ae04e",
   "metadata": {},
   "source": [
    "# This is a Template in python which gives generic functions which helps to do Data Analysis and to develop a Machine Learning Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a269aa3",
   "metadata": {},
   "source": [
    "## Steps involved "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae778a3",
   "metadata": {},
   "source": [
    "## 1- Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8c5e4",
   "metadata": {},
   "source": [
    "### 1(a)- Loading data\n",
    "### 1(b)- Data Analysis\n",
    "### 1(c)- Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41b503",
   "metadata": {},
   "source": [
    "## 2- Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba241cf",
   "metadata": {},
   "source": [
    "### 2(a)- Feature Selection\n",
    "### 2(b)- Model Building\n",
    "### 2(c)- Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f8053",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26dc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing Basic Modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "## importing feature selection modules\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import mutual_info_classifier,REF,REFCV\n",
    "\n",
    "## importing classification modules\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GuassianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaboostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "## importing regression modules\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "## importing classification evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have to install xgboost first, in order to import XGBClassifier\n",
    "## Command:  pip install xgboost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except:\n",
    "    print(\"XGBoost not Found. Install XGBoost from conda environment\")\n",
    "    print(\"Use following command: pip install xgboost\")\n",
    "    XGBClassifier = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except:\n",
    "    print(\"XGBoost not Found. Install XGBoost from conda environment\")\n",
    "    print(\"Use following command: conda install -c conda-forge lightgbm\")\n",
    "    lgb = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced51ce4",
   "metadata": {},
   "source": [
    "# 1- Load Data of different Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272082e",
   "metadata": {},
   "source": [
    "Following function will load the data irrespective of type. \n",
    "\n",
    "To use this function write: \n",
    "    \n",
    "    load_data('file_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39efeef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    def readcsv(file_name):\n",
    "        return pd.read_csv(file_name)\n",
    "    def readexcel(file_name):\n",
    "        return pd.read_excel(file_name)\n",
    "    def readjson(file_name):\n",
    "        return pd.read_json(file_name)\n",
    "    func_map = {\n",
    "        \"csv\": readcsv,\n",
    "        \"xls\": readexcel,\n",
    "        \"xlsx\": readexcel,\n",
    "        \"txt\": readcsv,\n",
    "        \"json\": readjson\n",
    "    }\n",
    "    \n",
    "    # default reader = readcsv\n",
    "    reader = func_map.get(\"csv\")\n",
    "    \n",
    "    for k,v in func_map.items():\n",
    "        if file_name.endswith(k):\n",
    "            reader = v\n",
    "            break\n",
    "    return reader(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b650d",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2c272",
   "metadata": {},
   "source": [
    "Following function will help you conduct Basic exploratory data analysis.\n",
    "\n",
    "To use this function write:\n",
    "\n",
    "    analyze(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05de822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(df):\n",
    "    print(\"Shape is:\\n\", df.shape)\n",
    "    print(\"Columns are:\\n\", df.columns)\n",
    "    print(\"Types are:\\n\", df.dtypes)\n",
    "    print(\"Statistical Analysis of Numerical Columns:\\n\", df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a3f70",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e3941",
   "metadata": {},
   "source": [
    "Following function will help you list down the count of missing values in the data.\n",
    "\n",
    "In order to use this function write:\n",
    "\n",
    "    missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b61e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(df):\n",
    "    #Missing values in Each column\n",
    "    print('Column Name' + '\\t\\t\\t' + 'Null Values')\n",
    "    return df.apply(lambda x: sum(x.isnull()),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d49c7f",
   "metadata": {},
   "source": [
    "## Columns which have missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76795158",
   "metadata": {},
   "source": [
    "Make a list of the variables that contain missing values.\n",
    "\n",
    "To use this function write:\n",
    "\n",
    "    list_missing_cols(dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3617fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_missing_cols(df):\n",
    "    vars_with_na = [var for var in df.columns if df[var].isnull().sum()>1]\n",
    "    return vars_with_na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc093e7",
   "metadata": {},
   "source": [
    "## Find Unique Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73023e2f",
   "metadata": {},
   "source": [
    "Following function will help you list down the unique identifiers in the data.\n",
    "\n",
    "To use this function write:\n",
    "    \n",
    "    find_unique(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cdd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique(df):\n",
    "    #Checking Unique Identifiers in our Data\n",
    "    print('Column Name' + '\\t\\t\\t' + 'IsUnique')\n",
    "    return df.apply(lambda x: x.is_unique,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf1b73d",
   "metadata": {},
   "source": [
    "## Numerical Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a3e52",
   "metadata": {},
   "source": [
    "Following function will return all the numerical columns in the dataset\n",
    "\n",
    "To use this function write:\n",
    "\n",
    "    list_numerical_cols(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d3bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_numerical_cols(df):\n",
    "    num_vars = [var for var in df.columns if df[var].dtypes != 'O']\n",
    "    return num_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87447307",
   "metadata": {},
   "source": [
    "## Numerical Columns Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc99f5",
   "metadata": {},
   "source": [
    "Plotting Histograms and boxplots for all numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d2b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numcolanalysis(df):\n",
    "    numcols = df.select_dtypes(include=np.number)\n",
    "    \n",
    "    # Box plot for numerical columns\n",
    "    for col in numcols:\n",
    "        fig = plt.figure(figsize = (5,5))\n",
    "        sb.boxplot(df[col], color='grey', linewidth=1)\n",
    "        plt.tight_layout()\n",
    "        plt.title(col)\n",
    "        plt.savefig(\"Numerical.png\")\n",
    "    \n",
    "    # Lets plot histograms for the numerical columns\n",
    "    df.hist(column=list(numcols.columns),bins=25, grid=False, figsize=(15,12),\n",
    "                 color='#86bf91', zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa657b",
   "metadata": {},
   "source": [
    "## String Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ceef7",
   "metadata": {},
   "source": [
    "This function will return all the string columns in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfc298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_categorical_cols(df):\n",
    "    cat_vars = [var for var in df.columns if df[var].dtypes == 'O']\n",
    "    return cat_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd52aa9",
   "metadata": {},
   "source": [
    "## Analysing String Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringcolanalysis(df):\n",
    "    stringcols = df.select_dtypes(exclude=[np.number, \"datetime64\"])\n",
    "    fig = plt.figure(figsize = (8,10))\n",
    "    for i,col in enumerate(stringcols):\n",
    "        fig.add_subplot(4,2,i+1)\n",
    "        fig.savefig('Categorical.png')\n",
    "        df[col].value_counts().plot(kind = 'bar', color='black' ,fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        plt.title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cb505",
   "metadata": {},
   "source": [
    "## Discrete Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9091cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_discrete(df):\n",
    "    temp_list = list_numerical_cols(df)\n",
    "    discrete_vars = [var for var in temp_list if len(df[var].unique())<20]\n",
    "    return discrete_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a206bbd6",
   "metadata": {},
   "source": [
    "## Continous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b21d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_continuous(df):\n",
    "    num_vars = list_numerical_cols(df)\n",
    "    discrete_vars = list_discrete(df)\n",
    "    cont_vars = [var for var in num_vars if var not in discrete_vars]\n",
    "    return cont_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4eb605",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1360b9",
   "metadata": {},
   "source": [
    "let's make boxplots to visualise outliers in the continuous variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb241dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers(df):\n",
    "    # As log does not take negative values, so we have to be careful and skip those variables\n",
    "    cont_vars = list_continuous(df)\n",
    "    for var in cont_vars:\n",
    "        if 0 in df[var].unique():\n",
    "            pass\n",
    "        else:\n",
    "            df[var] = np.log(df[var])\n",
    "            df.boxplot(column=var)\n",
    "            plt.title(var)\n",
    "            plt.ylabel(var)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b202e6f",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bbbf49",
   "metadata": {},
   "source": [
    "To perform correlation analysis over numerical columns this function act as a helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_anlysis(df):\n",
    "    # NOTE: If label column is non numeric, we need to 'encode' it before calling this function to have a better visibility\n",
    "    numcols = df.select_dtypes(include=np.number)\n",
    "    corr = numcols.corr()\n",
    "    ax = sb.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sb.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    "    )\n",
    "    \n",
    "    ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9354eede",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f868c5",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da025f",
   "metadata": {},
   "source": [
    "#### function to replace NA in categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908aa681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_categorical_na(df):\n",
    "    var_list = list_categorical_cols(df)\n",
    "    df[var_list] = df[var_list].fillna('Missing')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a9d97",
   "metadata": {},
   "source": [
    "#### function to replace NA with dedicated value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a62648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_categorical(df,value):\n",
    "    var_list = list_categorical_cols(df)\n",
    "    df[var_list] = df[var_list].fillna(value)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cbfa53",
   "metadata": {},
   "source": [
    "#### function to replace NA in numerical variables with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb864ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numerical_na(df):\n",
    "    vars_with_na = list_numerical_cols(df)\n",
    "    # replace the missing values\n",
    "    for var in vars_with_na:\n",
    "        df[var].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836821a",
   "metadata": {},
   "source": [
    "#### function to replace NA in numerical variables with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d73b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numerical_mode(df):\n",
    "    vars_with_na = list_numerical_cols(df)\n",
    "    # replace the missing values\n",
    "    for var in vars_with_na:\n",
    "    # calculate the mode\n",
    "        mode_val = df[var].mode()[0]\n",
    "        df[var].fillna(mode_val, inplace=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83a3b1b",
   "metadata": {},
   "source": [
    "#### function to replace NA in numerical variables with mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa269f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numerical_mean(df):\n",
    "    vars_with_na = list_numerical_cols(df)\n",
    "    # replace the missing values\n",
    "    for var in vars_with_na:\n",
    "    # calculate the mode\n",
    "        mean_val = df[var].mean()[0]\n",
    "        df[var].fillna(mean_val, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65509814",
   "metadata": {},
   "source": [
    "#### function to replace NA in numerical variables with dedicated value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff690e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numerical(df,value):\n",
    "    vars_with_na = list_numerical_cols(df)\n",
    "    # replace the missing values\n",
    "    for var in vars_with_na:\n",
    "        df[var].fillna(value, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca0e50",
   "metadata": {},
   "source": [
    "## Other Functions for data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30680d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtype_date(df, to_date):\n",
    "    # Deal with incorrect data in date column\n",
    "    for i in to_date:\n",
    "        df[i] = pd.to_datetime(df[i], errors='coerce')\n",
    "    return df\n",
    "            \n",
    "def dtype_numeric(df, to_numeric):\n",
    "    # Deal with incorrect data in numeric columns\n",
    "    for i in to_numeric:\n",
    "        df[i] = pd.to_numeric(df[i], errors='coerce')\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_useless_colums(df, cols_to_delete):\n",
    "    # Drop useless columns before dealing with missing values\n",
    "    for i in cols_to_delete:\n",
    "        df = df.drop(i, axis=1)\n",
    "    return df\n",
    "            \n",
    "    \n",
    "def drop_useless_rows(df):\n",
    "    # Drop useless rows before dealing with missing values\n",
    "    # Delete all rows containing 40% or more missing data\n",
    "    min_threshold = math.ceil(len(df.columns)*0.4)\n",
    "    df = df.dropna(thresh=min_threshold)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def drop_na_rows(df, cols_to_drop_na_rows):\n",
    "    # Drop rows with missing values for the columns specifically provided by the driver program\n",
    "    for i in cols_to_drop_na_rows:\n",
    "        df = df.drop(df[df[i].isnull()].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39bdea1",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37228888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_label_encoding(df, cols=[]):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in cols:\n",
    "        le.fit(df[i])\n",
    "        #print(list(le.classes_))\n",
    "        df[i] = le.transform(df[i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9e3f0",
   "metadata": {},
   "source": [
    "#### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5120183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringcolencoding(df, cols=[]):\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "    for col in cols:\n",
    "        integer_encoded = label_encoder.fit_transform(df[col])\n",
    "        ohe = onehot_encoder.fit_transform(integer_encoded.reshape(-1,1))\n",
    "        dfOneHot = pd.DataFrame(ohe, columns = [col+\"_\"+str(int(i)) for i in range(ohe.shape[1])])\n",
    "        df = pd.concat([df, dfOneHot], axis=1)\n",
    "        df = df.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2581caa",
   "metadata": {},
   "source": [
    "#### One Hot encoding with categorical dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringcolencoding_v2(df, cols=[]):\n",
    "    for col in cols:\n",
    "        df[col] = pd.Categorical(df[col])\n",
    "        dfDummies = pd.get_dummies(df[col], prefix = col)\n",
    "        df = pd.concat([df, dfDummies], axis=1)\n",
    "        df = df.drop(col, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de61609",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b8897",
   "metadata": {},
   "source": [
    "## X-y Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e088ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XYsplit(df, label_col):\n",
    "    y = df[label_col].copy()\n",
    "    X = df.drop(label_col,axis=1)\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148df6cf",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb3cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintestsplit(df,split,random=None, label_col=''):\n",
    "    #make a copy of the label column and store in y\n",
    "    y = df[label_col].copy()\n",
    "    #now delete the original\n",
    "    X = df.drop(label_col,axis=1)\n",
    "    #manual split\n",
    "    trainX, testX, trainY, testY= train_test_split(X, y, test_size=split, random_state=random)\n",
    "    return X, trainX, testX, trainY, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5857a2",
   "metadata": {},
   "source": [
    "## Cross-Validation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_kfold(X, y, split=10, random=None, shuffle=False):\n",
    "    \"\"\"\n",
    "    Generator function for KFold cross validation\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        trainX, testX = X.iloc[train_index], X.iloc[test_index] \n",
    "        trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield trainX,trainY,testX,testY\n",
    "    \n",
    "\n",
    "def cross_valid_repeated_kf(X, y, split=10, random=None, repeat=10):\n",
    "    \"\"\"\n",
    "    Generator function for Repeated KFold cross validation\n",
    "    \"\"\"\n",
    "    kf = RepeatedKFold(n_splits=split, random_state=random, n_repeats=repeat)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        trainX, testX = X.iloc[train_index], X.iloc[test_index] \n",
    "        trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield trainX,trainY,testX,testY\n",
    "        \n",
    "\n",
    "def cross_valid_stratified_kf(X, y, split=10, random=None, shuffle=False):\n",
    "    \"\"\"\n",
    "    Generator function for Stratified KFold cross validation\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=split, random_state=random, shuffle=shuffle)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        trainX, testX = X.iloc[train_index], X.iloc[test_index] \n",
    "        trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield trainX,trainY,testX,testY\n",
    "\n",
    "\n",
    "def cross_valid_strat_shuffle_kf(X, y, split=10, random=None):\n",
    "    \"\"\"\n",
    "    Generator function for StratifiedShuffle cross validation\n",
    "    \"\"\"\n",
    "    sss = StratifiedShuffleSplit(n_splits=split, random_state=random)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        trainX, testX = X.iloc[train_index], X.iloc[test_index] \n",
    "        trainY, testY = y.iloc[train_index], y.iloc[test_index]\n",
    "        yield trainX,trainY,testX,testY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412bdc7f",
   "metadata": {},
   "source": [
    "# 2- Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b9358",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b796b",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = KNeighborsClassifier()\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics(clf,testX,testY,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f061bc26",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogReg(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf  = LogisticRegression()\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics(clf,testX,testY,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca441c9",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2732d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics(clf,testX,testY,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7173a90",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83919104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GadientBoosting(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = GradientBoostingClassifier()\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics(clf,testX,testY,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c45d2",
   "metadata": {},
   "source": [
    "## Descion Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTree(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = DecisionTreeClassifier()\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics(clf,testX,testY,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984c2b7",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf  = RandomForestClassifier()\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics(clf,testX,testY,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae776685",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(trainX, testX, trainY, testY, svmtype=\"SVC\", verbose=True, clf=None):\n",
    "    # for one vs all\n",
    "    if not clf:\n",
    "        if svmtype == \"Linear\":\n",
    "            clf = svm.LinearSVC()\n",
    "        else:\n",
    "            clf = svm.SVC()\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics(clf,testX,testY,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009808e",
   "metadata": {},
   "source": [
    "## Naive Baeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = GaussianNB()\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics(clf,testX,testY,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38297e28",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a3809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XgBoost(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "    clf.fit(trainX,trainY)\n",
    "    return validationmetrics(clf,testX,testY,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1225e1",
   "metadata": {},
   "source": [
    "## Light Gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124bdd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LightGbm(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    d_train = lgb.Dataset(trainX, label=trainY)\n",
    "    params = {}\n",
    "    params['learning_rate'] = 0.003\n",
    "    params['boosting_type'] = 'gbdt'\n",
    "    params['objective'] = 'binary'\n",
    "    params['metric'] = 'binary_logloss'\n",
    "    params['sub_feature'] = 0.5\n",
    "    params['num_leaves'] = 10\n",
    "    params['min_data'] = 50\n",
    "    params['max_depth'] = 10\n",
    "    clf = lgb.train(params, d_train, 100)\n",
    "    return validationmetrics(clf,testX,testY,verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57deb12d",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f348379b",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearReg(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf  = LinearRegression()\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics_reg(clf, testX, testY, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be53ab3",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ba9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestReg(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = RandomForestRegressor(n_estimators=100)\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics_reg(clf, testX, testY, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a9e2b",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb30cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupportVectorRegression(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = SVR(kernel=\"rbf\")\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics_reg(clf, testX, testY, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c17d9f6",
   "metadata": {},
   "source": [
    "## Polynomial Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed11583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialReg(trainX, testX, trainY, testY, degree=3, verbose=True, clf=None):\n",
    "    poly = PolynomialFeatures(degree = degree)\n",
    "    X_poly = poly.fit_transform(trainX)\n",
    "    poly.fit(X_poly, trainY)\n",
    "    if not clf:\n",
    "        clf = LinearRegression() \n",
    "    clf.fit(X_poly, trainY)\n",
    "    return validationmetrics_reg(clf, poly.fit_transform(testX), testY, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63e533",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea279f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreeReg(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = DecisionTreeRegressor()\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics_reg(clf, testX, testY, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83901e",
   "metadata": {},
   "source": [
    "## Adaboost rehression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBooostReg(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics_reg(clf, testX, testY, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c41fc",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aebd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientBoostingReg(trainX, testX, trainY, testY, verbose=True, clf=None):\n",
    "    if not clf:\n",
    "        clf = GradientBoostingRegressor()\n",
    "    clf.fit(trainX , trainY)\n",
    "    return validationmetrics_reg(clf, testX, testY, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede69cfb",
   "metadata": {},
   "source": [
    "# 3- selecting the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da6767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supported_algorithms(classification=False,regression=False):\n",
    "    if classification:\n",
    "        algo_list = get_supported_classification_algorithms()\n",
    "    else:\n",
    "        algo_list = get_supported_regression_algorithms()\n",
    "    return algo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_supported_regression_algorithms():\n",
    "    covered_algorithms = [LinearReg, RandomForestReg, PolynomialReg, SupportVectorRegression,\n",
    "                          DecisionTreeReg, GradientBoostingReg, AdaBooostReg, VotingReg]\n",
    "    return covered_algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(df,label_col,feature_list=[]):\n",
    "    algo_list = get_supported_algorithms()\n",
    "    \"\"\"\n",
    "    Run Algorithms with manual split\n",
    "    \"\"\"\n",
    "    # Lets make a copy of dataframe and work on that to be on safe side \n",
    "    _df = df.copy()\n",
    "    \n",
    "    if feature_list:\n",
    "        impftrs = feature_list\n",
    "        impftrs.append(label_col)\n",
    "        _df = _df[impftrs]\n",
    "    \n",
    "    _df, trainX, testX, trainY, testY = traintestsplit(_df, 0.2, 91, label_col=label_col)\n",
    "    algo_model_map = {}\n",
    "    for algo in algo_list:\n",
    "        print(\"============ \" + algo.__name__ + \" ===========\")\n",
    "        res = algo(trainX, testX, trainY, testY)\n",
    "        algo_model_map[algo.__name__] = res.get(\"model_obj\", None)\n",
    "        print (\"============================== \\n\")\n",
    "    \n",
    "    return algo_model_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8febd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select features with importance >=threshold\n",
    "def MachineLearningwithRFFS(df, label_col, threshold=5, algo_list=get_supported_algorithms(), regression=False):\n",
    "    # lets create a copy of this dataframe and perform feature selection analysis over that\n",
    "    df_cpy = df.copy()\n",
    "    df_cpy, trainX, testX, trainY, testY = traintestsplit(df_cpy, 0.2, 91, label_col=label_col)\n",
    "    res = RFfeatureimportance(df_cpy, trainX, testX, trainY, testY, trees=10, regression=regression)\n",
    "    \n",
    "    impftrs = list(res[res > threshold].keys())\n",
    "    #impftrs.append(label_col)\n",
    "    \n",
    "    print (\"Selected Features =\" + str(impftrs))\n",
    "    print(df.shape)\n",
    "    results = run_algorithms(df, label_col, algo_list=algo_list, feature_list=impftrs)\n",
    "    return {\"selected_features\": impftrs, \"results\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With stratified kfold validation support\n",
    "def find_best_model_cv(df, label_col, algo_list=get_supported_algorithms(), feature_list=[], cross_valid_method=cross_valid_stratified_kf):\n",
    "    \"\"\"\n",
    "    Run Algorithms with cross validation\n",
    "    \"\"\"\n",
    "    _df = df.copy()\n",
    "    X,y = XYsplit(_df, label_col)\n",
    "    \n",
    "    # Select features if specified by driver program\n",
    "    if feature_list:\n",
    "        X = X[feature_list]\n",
    "    \n",
    "    result = {}\n",
    "    algo_model_map = {}\n",
    "    for algo in algo_list:\n",
    "        clf = None\n",
    "        result[algo.__name__] = dict()\n",
    "        for trainX,trainY,testX,testY  in cross_valid_method(X, y, split=10):\n",
    "            res_algo = algo(trainX, testX, trainY, testY, verbose=False, clf=clf)\n",
    "            # Get trained model so we could use it again in the next iteration\n",
    "            clf = res_algo.get(\"model_obj\", None)\n",
    "            \n",
    "            for k,v in res_algo.items():\n",
    "                if k == \"model_obj\":\n",
    "                    continue\n",
    "                if k not in result[algo.__name__].keys():\n",
    "                    result[algo.__name__][k] = list()\n",
    "                result[algo.__name__][k].append(v)\n",
    "                \n",
    "        algo_model_map[algo.__name__] = clf\n",
    "        \n",
    "    \n",
    "    score_map = dict()\n",
    "    # let take average scores for all folds now\n",
    "    for algo, metrics in result.items():\n",
    "        print(\"============ \" + algo + \" ===========\")\n",
    "        score_map[algo] = dict()\n",
    "        for metric_name, score_lst in metrics.items():\n",
    "            score_map[algo][metric_name] = np.mean(score_lst)\n",
    "        print(score_map[algo])\n",
    "        print (\"============================== \\n\")\n",
    "        score_map[algo][\"model_obj\"] = algo_model_map[algo]\n",
    "    \n",
    "    return score_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9981cc9d",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ae90c3",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validationmetrics(model,testX,testY, verbose=True):\n",
    "    \n",
    "    predictions = model.predict(testX)\n",
    "    \n",
    "    if model.__class__.__module__.startswith('lightgbm'):\n",
    "        for i in range(0, predictions.shape[0]):\n",
    "            predictions[i]= 1 if predictions[i] >= 0.5 else 0\n",
    "    \n",
    "    #Accuracy\n",
    "    accuracy = accuracy_score(testY, predictions)*100\n",
    "    \n",
    "    #Precision\n",
    "    precision = precision_score(testY, predictions,pos_label=1,labels=[0,1])*100\n",
    "    \n",
    "    #Recall\n",
    "    recall = recall_score(testY, predictions,pos_label=1,labels=[0,1])*100\n",
    "    \n",
    "    #get FPR (specificity) and TPR (sensitivity)\n",
    "    fpr , tpr, _ = roc_curve(testY, predictions)\n",
    "    \n",
    "    #AUC\n",
    "    auc_val = auc(fpr, tpr)\n",
    "    \n",
    "    #F-Score\n",
    "    f_score = f1_score(testY, predictions)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Prediction Vector: \\n\", predictions)\n",
    "        print(\"Accuracy: \\n\", accuracy)\n",
    "        print(\"Precision of event Happening: \\n\", precision)\n",
    "        print(\"Recall of event Happening: \\n\", recall)\n",
    "        print(\"AUC: \\n\",auc_val)\n",
    "        print(\"F-Score:\\n\", f_score)\n",
    "        #confusion Matrix\n",
    "        print(\"Confusion Matrix: \\n\", confusion_matrix(testY, predictions,labels=[0,1]))\n",
    "    \n",
    "    res_map = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"auc_val\": auc_val,\n",
    "                \"f_score\": f_score,\n",
    "                \"model_obj\": model\n",
    "              }\n",
    "    return res_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a17088a",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d74bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validationmetrics_reg(model,testX,testY, verbose=True):\n",
    "    predictions = model.predict(testX)\n",
    "    \n",
    "    # R-squared\n",
    "    r2 = r2_score(testY,predictions)\n",
    "    \n",
    "    # Adjusted R-squared\n",
    "    r2_adjusted = 1-(1-r2)*(testX.shape[0]-1)/(testX.shape[0]-testX.shape[1]-1)\n",
    "    \n",
    "    # MSE\n",
    "    mse = mean_squared_error(testY,predictions)\n",
    "    \n",
    "    #RMSE\n",
    "    rmse = math.sqrt(mse)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"R-Squared Value: \", r2)\n",
    "        print(\"Adjusted R-Squared: \", r2_adjusted)\n",
    "        print(\"RMSE: \", rmse)\n",
    "    \n",
    "    res_map = {\n",
    "                \"r2\": r2,\n",
    "                \"r2_adjusted\": r2_adjusted,\n",
    "                \"rmse\": rmse,\n",
    "                \"model_obj\": model\n",
    "              }\n",
    "    return res_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b82dac",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec8bfd",
   "metadata": {},
   "source": [
    "## Random Forest Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFfeatureimportance(df, trainX, testX, trainY, testY, trees=10, random=None, regression=False):\n",
    "    if regression:\n",
    "        clf  = RandomForestRegressor(n_estimators=trees, random_state=random)\n",
    "    else:\n",
    "        clf  = RandomForestClassifier(n_estimators=trees, random_state=random)\n",
    "    clf.fit(trainX,trainY)\n",
    "    #validationmetrics(clf,testX,testY)\n",
    "    res = pd.Series(clf.feature_importances_, index=df.columns.values).sort_values(ascending=False)*100\n",
    "    print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631fa1bc",
   "metadata": {},
   "source": [
    "## Run Algo with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441298ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select features with importance >=threshold\n",
    "def MachineLearningwithRFFS(df, label_col, threshold=5, algo_list=get_supported_algorithms(), regression=False):\n",
    "    # lets create a copy of this dataframe and perform feature selection analysis over that\n",
    "    df_cpy = df.copy()\n",
    "    df_cpy, trainX, testX, trainY, testY = traintestsplit(df_cpy, 0.2, 91, label_col=label_col)\n",
    "    res = RFfeatureimportance(df_cpy, trainX, testX, trainY, testY, trees=10, regression=regression)\n",
    "    \n",
    "    impftrs = list(res[res > threshold].keys())\n",
    "    #impftrs.append(label_col)\n",
    "    \n",
    "    print (\"Selected Features =\" + str(impftrs))\n",
    "    print(df.shape)\n",
    "    results = run_algorithms(df, label_col, algo_list=algo_list, feature_list=impftrs)\n",
    "    return {\"selected_features\": impftrs, \"results\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71261fe",
   "metadata": {},
   "source": [
    "# With Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af05a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select features with importance >=threshold\n",
    "def MachineLearningwithRFFS_CV(df, label_col, threshold=5, algo_list=get_supported_algorithms(), regression=False):\n",
    "    # lets create a copy of this dataframe and perform feature selection analysis over that\n",
    "    df_cpy = df.copy()\n",
    "    df_cpy, trainX, testX, trainY, testY = traintestsplit(df_cpy, 0.2, 91, label_col=label_col)\n",
    "    res = RFfeatureimportance(df_cpy, trainX, testX, trainY, testY,\n",
    "                              trees=10, regression=regression)\n",
    "\n",
    "    impftrs = list(res[res > threshold].keys())\n",
    "    \n",
    "    print (\"Selected Features =\" + str(impftrs))\n",
    "    print(df.shape)\n",
    "    if regression:\n",
    "        cross_valid_method = cross_valid_kfold\n",
    "    else:\n",
    "        cross_valid_method = cross_valid_stratified_kf\n",
    "    results = run_algorithms_cv(df, label_col, algo_list=algo_list, feature_list=impftrs, cross_valid_method=cross_valid_method)\n",
    "    return {\"selected_features\": impftrs, \"results\": results}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15812069",
   "metadata": {},
   "source": [
    "## Mutual Information Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f907e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the important features given by MIFS\n",
    "def mutualinformation(df, label_col, regression=False):\n",
    "    df_cpy = df.copy()\n",
    "    y = df_cpy[label_col].copy()\n",
    "    X = df_cpy.drop(label_col,axis=1)\n",
    "    if regression:\n",
    "        mutual_info = mutual_info_regression(X,y,random_state=35)\n",
    "    else:\n",
    "        mutual_info = mutual_info_classif(X,y,random_state=35)\n",
    "    results = pd.Series(mutual_info, index=X.columns).sort_values(ascending=False)*100\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d19af5",
   "metadata": {},
   "source": [
    "## Without Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726932e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features with importance >=threshold\n",
    "def MachineLearningwithMIFS(df, label_col, threshold=5, algo_list=get_supported_algorithms(), regression=False):\n",
    "    \n",
    "    # lets create a copy of this dataframe and perform feature selection analysis over that\n",
    "    df_cpy = df.copy()\n",
    "    res = mutualinformation(df_cpy, label_col=label_col, regression=regression)\n",
    "    \n",
    "    #include all selected features in impftrs\n",
    "    impftrs = list(res[res > threshold].keys())\n",
    "    \n",
    "    print (\"Selected Features =\" + str(impftrs))\n",
    "    \n",
    "    results = run_algorithms(df, label_col, algo_list=algo_list, feature_list=impftrs)\n",
    "    return {\"selected_features\": impftrs, \"results\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae29cf",
   "metadata": {},
   "source": [
    "## With Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b10126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select features with importance >=threshold\n",
    "def MachineLearningwithMIFS_CV(df, label_col, threshold=5, algo_list=get_supported_algorithms(), regression=False):\n",
    "    \n",
    "    # lets create a copy of this dataframe and perform feature selection analysis over that\n",
    "    df_cpy = df.copy()\n",
    "    res = mutualinformation(df_cpy, label_col=label_col, regression=regression)\n",
    "    \n",
    "    #include all selected features in impftrs\n",
    "    impftrs = list(res[res > threshold].keys())\n",
    "    \n",
    "    print (\"Selected Features =\" + str(impftrs))\n",
    "    if regression:\n",
    "        cross_valid_method = cross_valid_kfold\n",
    "    else:\n",
    "        cross_valid_method = cross_valid_stratified_kf\n",
    "    results = run_algorithms_cv(df, label_col, algo_list=algo_list, feature_list=impftrs, cross_valid_method=cross_valid_method)\n",
    "    return {\"selected_features\": impftrs, \"results\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0262b3f8",
   "metadata": {},
   "source": [
    "## Recursive Elimination Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67acfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenericREFS(df, label_col,\n",
    "                algo_list=get_supported_algorithms(),\n",
    "                re_algo=RandomForestClassifier,\n",
    "                **kwargs):\n",
    "    \n",
    "    X,y = XYsplit(df, label_col)\n",
    "    clf = re_algo(**kwargs)\n",
    "    selector = RFE(estimator=clf, step=1)\n",
    "    selector = selector.fit(X,y)\n",
    "    feature_list = X.columns[selector.support_].tolist()\n",
    "    \n",
    "    results = run_algorithms(df, label_col, algo_list=algo_list, feature_list=feature_list)\n",
    "    return {\"selected_features\": feature_list, \"results\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9d67a",
   "metadata": {},
   "source": [
    "## With Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46551a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenericREFS_CV(df, label_col,\n",
    "                algo_list=get_supported_algorithms(),\n",
    "                regression=False,\n",
    "                re_algo=RandomForestClassifier,\n",
    "                **kwargs):\n",
    "    \n",
    "    X,y = XYsplit(df, label_col)\n",
    "    clf = re_algo(**kwargs)\n",
    "    selector = RFECV(estimator=clf, step=1, cv=10)\n",
    "    selector = selector.fit(X,y)\n",
    "    feature_list = X.columns[selector.support_].tolist()\n",
    "    if regression:\n",
    "        cross_valid_method = cross_valid_kfold\n",
    "    else:\n",
    "        cross_valid_method = cross_valid_stratified_kf\n",
    "    results = run_algorithms_cv(df, label_col, algo_list=algo_list, feature_list=feature_list, cross_valid_method=cross_valid_method)\n",
    "    return {\"selected_features\": feature_list, \"results\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to provide list of classification algorithms to be used for recursive elimination feature selection\n",
    "def get_supported_algorithms_refs():\n",
    "    algo_list = [LogisticRegression, GradientBoostingClassifier, AdaBoostClassifier,\n",
    "                          DecisionTreeClassifier, RandomForestClassifier]\n",
    "    return algo_list\n",
    "\n",
    "# Helper function to provide list of regression algorithms to be used for recursive elimination feature selection\n",
    "def get_supported_reg_algorithms_refs():\n",
    "    algo_list = [LinearRegression, RandomForestRegressor,\n",
    "                 DecisionTreeRegressor, GradientBoostingRegressor, AdaBoostRegressor]\n",
    "    return algo_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d4661",
   "metadata": {},
   "source": [
    "## Feature Selection Using PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_FS(df, label_col, n_components, algo_list=get_supported_algorithms()):\n",
    "    df_cpy = df.copy()\n",
    "    X,y = XYsplit(df_cpy, label_col)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # First we need to normalize the data\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    \n",
    "    # Now perform PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    algo_model_map = {}\n",
    "    # At this stage we apply alogorithms\n",
    "    for algo in algo_list:\n",
    "        print(\"============ \" + algo.__name__ + \" ===========\")\n",
    "        res = algo(X_train, X_test, y_train, y_test)\n",
    "        algo_model_map[algo.__name__] = res.get(\"model_obj\", None)\n",
    "        \n",
    "        print(\"============================== \\n\")\n",
    "    return {\"n_components\": n_components, \"results\": algo_model_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c984f4c",
   "metadata": {},
   "source": [
    "## With Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896efe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_FS_CV(df, label_col, n_components, algo_list=get_supported_algorithms(), regression=False):\n",
    "    df_cpy = df.copy()\n",
    "    X,y = XYsplit(df_cpy, label_col)\n",
    "    \n",
    "    cross_valid_method = cross_valid_kfold if regression else cross_valid_stratified_kf \n",
    "    result = {}\n",
    "    algo_model_map = {}\n",
    "    for algo in algo_list:\n",
    "        clf = None\n",
    "        result[algo.__name__] = dict()\n",
    "        for X_train,y_train,X_test,y_test in cross_valid_method(X, y, split=10):\n",
    "            # First we need to normalize the data\n",
    "            sc = StandardScaler()\n",
    "            X_train = sc.fit_transform(X_train)\n",
    "            X_test = sc.transform(X_test)\n",
    "            \n",
    "            # Now perform PCA\n",
    "            pca = PCA(n_components=n_components)\n",
    "            X_train = pca.fit_transform(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "            \n",
    "            # apply algo on this fold and save result for later usage\n",
    "            res_algo = algo(X_train, X_test, y_train, y_test, verbose=False, clf=clf)\n",
    "            # Get trained model so we could use it again in the next iteration\n",
    "            clf = res_algo.get(\"model_obj\", None)\n",
    "            \n",
    "            for k,v in res_algo.items():\n",
    "                if k == \"model_obj\":\n",
    "                    continue\n",
    "                if k not in result[algo.__name__].keys():\n",
    "                    result[algo.__name__][k] = list()\n",
    "                result[algo.__name__][k].append(v)\n",
    "            \n",
    "        algo_model_map[algo.__name__] = clf\n",
    "        \n",
    "    \n",
    "    score_map = dict()\n",
    "    # let take average scores for all folds now\n",
    "    for algo, metrics in result.items():\n",
    "        print(\"============ \" + algo + \" ===========\")\n",
    "        score_map[algo] = dict()\n",
    "        for metric_name, score_lst in metrics.items():\n",
    "            score_map[algo][metric_name] = np.mean(score_lst)\n",
    "        print(score_map[algo])\n",
    "        print (\"============================== \\n\")\n",
    "        score_map[algo][\"model_obj\"] =  algo_model_map[algo]\n",
    "    return {\"n_components\": n_components, \"results\": algo_model_map}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
